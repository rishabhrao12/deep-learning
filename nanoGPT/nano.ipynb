{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4  # how many independent sequences will be processed in parallel?\n",
    "block_size = 8  # what is the maximum context length for prediction?\n",
    "max_iters = 5000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1003854]) torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as file:\n",
    "  text = file.read()\n",
    "\n",
    "# Unqiue characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Create mapping for characters\n",
    "itos = {i:s for i, s in enumerate(chars)}\n",
    "stoi = {s:i for i, s in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # take a string output a list of integer\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # take a list of integer output a string\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(train_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "  # generate small batch of data of inputs x and target y\n",
    "  data = train_data if split == 'train' else val_data\n",
    "  ix = torch.randint(len(train_data) - block_size, (batch_size, ))\n",
    "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "  y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"one head of self attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)     # (B, T, C)\n",
    "        q = self.query(x)   # (B, T, C)\n",
    "        # compute attention scores ('affinities)\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5     # (B, T, C) @ (B, C, T) --> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))    # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)    # (B, T, T)\n",
    "        # perform weighted aggregation of values\n",
    "        v = self.value(x)\n",
    "        out = wei @ v   # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.poition_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_head = Head(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B, T) tensors of integers\n",
    "        token_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_emb = self.poition_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "        x = token_emb + pos_emb # (B, T, C)\n",
    "        x = self.sa_head(x)\n",
    "        logits = self.lm_head(x) \n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to last block size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus on last time step\n",
    "            logits = logits[:, -1, :]   # becomes (B, T)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # sample from probs\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [0] at entry 0 and [8] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate)\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39miter\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iters):\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m     \u001b[39m# every once in a while evaluate loss on train and val split\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     losses \u001b[39m=\u001b[39m estimate_loss()\n\u001b[1;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(losses)\n\u001b[1;32m     13\u001b[0m     \u001b[39m# sample a batch of data\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/dl/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m, in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m losses \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(eval_iters)\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(eval_iters):\n\u001b[0;32m----> 8\u001b[0m     X, Y \u001b[39m=\u001b[39m get_batch(split)\n\u001b[1;32m      9\u001b[0m     logits, loss \u001b[39m=\u001b[39m model(X, Y)\n\u001b[1;32m     10\u001b[0m     losses[k] \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m      3\u001b[0m data \u001b[39m=\u001b[39m train_data \u001b[39mif\u001b[39;00m split \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m val_data\n\u001b[1;32m      4\u001b[0m ix \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39mlen\u001b[39m(train_data) \u001b[39m-\u001b[39m block_size, (batch_size, ))\n\u001b[0;32m----> 5\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack([data[i:i\u001b[39m+\u001b[39;49mblock_size] \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m ix])\n\u001b[1;32m      6\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([data[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m: i\u001b[39m+\u001b[39mblock_size\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m ix])\n\u001b[1;32m      7\u001b[0m \u001b[39mreturn\u001b[39;00m x, y\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [0] at entry 0 and [8] at entry 1"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# create a pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate loss on train and val split\n",
    "    losses = estimate_loss()\n",
    "    print(losses)\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    break\n",
    "\n",
    "# generate from model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
